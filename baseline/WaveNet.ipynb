{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syjMmwOEa-uk"
   },
   "source": [
    "# Tacotron2: WaveNet-basd text-to-speech demo\n",
    "\n",
    "- Tacotron2 (mel-spectrogram prediction part): https://github.com/Rayhane-mamah/Tacotron-2\n",
    "- WaveNet: https://github.com/r9y9/wavenet_vocoder\n",
    "\n",
    "This is a proof of concept for Tacotron2 text-to-speech synthesis. Models used here were trained on [LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/).\n",
    "\n",
    "**Notice**: The waveform generation is super slow since it implements naive autoregressive generation. It doesn't use parallel generation method described in [Parallel WaveNet](https://arxiv.org/abs/1711.10433). \n",
    "\n",
    "**Estimated time to complete**: 2 ~ 3 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7R_1MpFc3Za"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlLC7Q7Us8go"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists, join, expanduser\n",
    "\n",
    "os.chdir(expanduser(\"~\"))\n",
    "\n",
    "wavenet_dir = \"wavenet_vocoder\"\n",
    "if not exists(wavenet_dir):\n",
    "  ! git clone https://github.com/r9y9/$wavenet_dir\n",
    "  ! cd wavenet_vocoder && git checkout v0.1.1 && cd -\n",
    "    \n",
    "taco2_dir = \"Tacotron-2\"\n",
    "if not exists(taco2_dir):\n",
    "  ! git clone https://github.com/r9y9/$taco2_dir\n",
    "  ! cd $taco2_dir && git checkout -B wavenet3 origin/wavenet3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBFfji_Avluz"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "#! pip install -q -U \"tensorflow<=1.9.0\"\n",
    "! pip install -q -U \"keras==2.2.4\"\n",
    "! pip install -q -U \"numpy<1.16\"\n",
    "! pip install -q -U \"pysptk<=0.1.14\"\n",
    "\n",
    "os.chdir(join(expanduser(\"~\"), taco2_dir))\n",
    "! pip install -q -r requirements.txt\n",
    "\n",
    "os.chdir(join(expanduser(\"~\"), wavenet_dir))\n",
    "! pip install -q -e '.[train]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43Z8J-xEu3xn"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.1\n",
    "!pip install torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15p8phXx6nxe"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "import pysptk\n",
    "import numpy as np\n",
    "tensorflow.__version__, pysptk.__version__, np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fZo1X7ac_Tp"
   },
   "source": [
    "### Download pretrained models\n",
    "\n",
    "#### Tacotron2 (mel-spectrogram prediction part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sau06KhizkoD"
   },
   "outputs": [],
   "source": [
    "os.chdir(join(expanduser(\"~\"), taco2_dir))\n",
    "! mkdir -p logs-Tacotron\n",
    "if not exists(\"logs-Tacotron/pretrained\"):\n",
    "  ! curl -O -L \"https://www.dropbox.com/s/vx7y4qqs732sqgg/pretrained.tar.gz\"\n",
    "  ! tar xzvf pretrained.tar.gz\n",
    "  ! mv pretrained logs-Tacotron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4tWl_hfdXdh"
   },
   "source": [
    "#### WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2kwJ-t_ykXZ"
   },
   "outputs": [],
   "source": [
    "os.chdir(join(expanduser(\"~\"), wavenet_dir))\n",
    "wn_preset = \"20180510_mixture_lj_checkpoint_step000320000_ema.json\"\n",
    "wn_checkpoint_path = \"20180510_mixture_lj_checkpoint_step000320000_ema.pth\"\n",
    "\n",
    "if not exists(wn_preset):\n",
    "  !curl -O -L \"https://www.dropbox.com/s/0vsd7973w20eskz/20180510_mixture_lj_checkpoint_step000320000_ema.json\"\n",
    "if not exists(wn_checkpoint_path):\n",
    "  !curl -O -L \"https://www.dropbox.com/s/zdbfprugbagfp2w/20180510_mixture_lj_checkpoint_step000320000_ema.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LeTMHHFdcmS"
   },
   "outputs": [],
   "source": [
    "os.chdir(join(expanduser(\"~\"), taco2_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF1mh1Jvdp0a"
   },
   "source": [
    "## Waveform synthesis by WaveNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY_MfE0m8Ese"
   },
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTmp0T0G3lU0"
   },
   "outputs": [],
   "source": [
    "os.chdir(join(expanduser(\"~\"), wavenet_dir))\n",
    "\n",
    "# Setup WaveNet vocoder hparams\n",
    "from hparams import hparams\n",
    "with open(wn_preset) as f:\n",
    "    hparams.parse_json(f.read())\n",
    "\n",
    "# Setup WaveNet vocoder\n",
    "from train import build_model\n",
    "from synthesis import wavegen\n",
    "import torch\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "model = build_model().to(device)\n",
    "\n",
    "print(\"Load checkpoint from {}\".format(wn_checkpoint_path))\n",
    "checkpoint = torch.load(wn_checkpoint_path)\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9BO7IES7Htp"
   },
   "outputs": [],
   "source": [
    "mels2=load_mel('/content/LJ001-0153real.wav')\n",
    "mels2=mels2.permute(0,2,1)\n",
    "mels2=mels2#.cuda()\n",
    "start = timeit.default_timer()\n",
    "waveform = wavegen(model, c=mels2, fast=True)\n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)\n",
    "IPython.display.display(Audio(waveform, rate=hparams.sample_rate))\n",
    "write(\"LJ001-0153glow.wav\", 22050, waveform) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E3v7DOywmWv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcaOkwP9wmZe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXeuF4oLwmcG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVnQOAdDwme-"
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib==2.1.0\n",
    "!pip install tensorflow==1.15.2\n",
    "!pip install numpy==1.13.3\n",
    "!pip install inflect==0.2.5\n",
    "!pip install librosa==0.6.0\n",
    "!pip install scipy==1.0.0\n",
    "!pip install Unidecode==1.0.22\n",
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R9Pg40QxdXo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask\n",
    "\n",
    "\n",
    "def load_wav_to_torch(full_path):\n",
    "    sampling_rate, data = read(full_path)\n",
    "    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n",
    "\n",
    "\n",
    "def load_filepaths_and_text(filename, split=\"|\"):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        filepaths_and_text = [line.strip().split(split) for line in f]\n",
    "    return filepaths_and_text\n",
    "\n",
    "\n",
    "def to_gpu(x):\n",
    "    x = x.contiguous()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda(non_blocking=True)\n",
    "    return torch.autograd.Variable(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xr-Bpqm7w_kO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import get_window\n",
    "import librosa.util as librosa_util\n",
    "\n",
    "\n",
    "def window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n",
    "                     n_fft=800, dtype=np.float32, norm=None):\n",
    "    \"\"\"\n",
    "    # from librosa 0.6\n",
    "    Compute the sum-square envelope of a window function at a given hop length.\n",
    "\n",
    "    This is used to estimate modulation effects induced by windowing\n",
    "    observations in short-time fourier transforms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window : string, tuple, number, callable, or list-like\n",
    "        Window specification, as in `get_window`\n",
    "\n",
    "    n_frames : int > 0\n",
    "        The number of analysis frames\n",
    "\n",
    "    hop_length : int > 0\n",
    "        The number of samples to advance between frames\n",
    "\n",
    "    win_length : [optional]\n",
    "        The length of the window function.  By default, this matches `n_fft`.\n",
    "\n",
    "    n_fft : int > 0\n",
    "        The length of each analysis frame.\n",
    "\n",
    "    dtype : np.dtype\n",
    "        The data type of the output\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n",
    "        The sum-squared envelope of the window function\n",
    "    \"\"\"\n",
    "    if win_length is None:\n",
    "        win_length = n_fft\n",
    "\n",
    "    n = n_fft + hop_length * (n_frames - 1)\n",
    "    x = np.zeros(n, dtype=dtype)\n",
    "\n",
    "    # Compute the squared window at the desired length\n",
    "    win_sq = get_window(window, win_length, fftbins=True)\n",
    "    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n",
    "    win_sq = librosa_util.pad_center(win_sq, n_fft)\n",
    "\n",
    "    # Fill the envelope\n",
    "    for i in range(n_frames):\n",
    "        sample = i * hop_length\n",
    "        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n",
    "    return x\n",
    "\n",
    "\n",
    "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    magnitudes: spectrogram magnitudes\n",
    "    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n",
    "    \"\"\"\n",
    "\n",
    "    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n",
    "    angles = angles.astype(np.float32)\n",
    "    angles = torch.autograd.Variable(torch.from_numpy(angles))\n",
    "    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        _, angles = stft_fn.transform(signal)\n",
    "        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n",
    "    return signal\n",
    "\n",
    "\n",
    "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor\n",
    "    \"\"\"\n",
    "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
    "\n",
    "\n",
    "def dynamic_range_decompression(x, C=1):\n",
    "    \"\"\"\n",
    "    PARAMS\n",
    "    ------\n",
    "    C: compression factor used to compress\n",
    "    \"\"\"\n",
    "    return torch.exp(x) / C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEVYUHU8xFDP"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy.signal import get_window\n",
    "from librosa.util import pad_center, tiny\n",
    "class STFT(torch.nn.Module):\n",
    "    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n",
    "    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n",
    "                 window='hann'):\n",
    "        super(STFT, self).__init__()\n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.window = window\n",
    "        self.forward_transform = None\n",
    "        scale = self.filter_length / self.hop_length\n",
    "        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n",
    "\n",
    "        cutoff = int((self.filter_length / 2 + 1))\n",
    "        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n",
    "                                   np.imag(fourier_basis[:cutoff, :])])\n",
    "\n",
    "        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n",
    "        inverse_basis = torch.FloatTensor(\n",
    "            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n",
    "\n",
    "        if window is not None:\n",
    "            assert(filter_length >= win_length)\n",
    "            # get window and zero center pad it to filter_length\n",
    "            fft_window = get_window(window, win_length, fftbins=True)\n",
    "            fft_window = pad_center(fft_window, filter_length)\n",
    "            fft_window = torch.from_numpy(fft_window).float()\n",
    "\n",
    "            # window the bases\n",
    "            forward_basis *= fft_window\n",
    "            inverse_basis *= fft_window\n",
    "\n",
    "        self.register_buffer('forward_basis', forward_basis.float())\n",
    "        self.register_buffer('inverse_basis', inverse_basis.float())\n",
    "\n",
    "    def transform(self, input_data):\n",
    "        num_batches = input_data.size(0)\n",
    "        num_samples = input_data.size(1)\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # similar to librosa, reflect-pad the input\n",
    "        input_data = input_data.view(num_batches, 1, num_samples)\n",
    "        input_data = F.pad(\n",
    "            input_data.unsqueeze(1),\n",
    "            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n",
    "            mode='reflect')\n",
    "        input_data = input_data.squeeze(1)\n",
    "\n",
    "        forward_transform = F.conv1d(\n",
    "            input_data,\n",
    "            Variable(self.forward_basis, requires_grad=False),\n",
    "            stride=self.hop_length,\n",
    "            padding=0)\n",
    "\n",
    "        cutoff = int((self.filter_length / 2) + 1)\n",
    "        real_part = forward_transform[:, :cutoff, :]\n",
    "        imag_part = forward_transform[:, cutoff:, :]\n",
    "\n",
    "        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n",
    "        phase = torch.autograd.Variable(\n",
    "            torch.atan2(imag_part.data, real_part.data))\n",
    "\n",
    "        return magnitude, phase\n",
    "\n",
    "    def inverse(self, magnitude, phase):\n",
    "        recombine_magnitude_phase = torch.cat(\n",
    "            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n",
    "\n",
    "        inverse_transform = F.conv_transpose1d(\n",
    "            recombine_magnitude_phase,\n",
    "            Variable(self.inverse_basis, requires_grad=False),\n",
    "            stride=self.hop_length,\n",
    "            padding=0)\n",
    "\n",
    "        if self.window is not None:\n",
    "            window_sum = window_sumsquare(\n",
    "                self.window, magnitude.size(-1), hop_length=self.hop_length,\n",
    "                win_length=self.win_length, n_fft=self.filter_length,\n",
    "                dtype=np.float32)\n",
    "            # remove modulation effects\n",
    "            approx_nonzero_indices = torch.from_numpy(\n",
    "                np.where(window_sum > tiny(window_sum))[0])\n",
    "            window_sum = torch.autograd.Variable(\n",
    "                torch.from_numpy(window_sum), requires_grad=False)\n",
    "            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n",
    "            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n",
    "\n",
    "            # scale by hop ratio\n",
    "            inverse_transform *= float(self.filter_length) / self.hop_length\n",
    "\n",
    "        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n",
    "        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n",
    "\n",
    "        return inverse_transform\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self.magnitude, self.phase = self.transform(input_data)\n",
    "        reconstruction = self.inverse(self.magnitude, self.phase)\n",
    "        return reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qhkwdcHw7S3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from librosa.filters import mel as librosa_mel_fn\n",
    "class TacotronSTFT(torch.nn.Module):\n",
    "    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n",
    "                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n",
    "                 mel_fmax=8000.0):\n",
    "        super(TacotronSTFT, self).__init__()\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.stft_fn = STFT(filter_length, hop_length, win_length)\n",
    "        mel_basis = librosa_mel_fn(\n",
    "            sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)\n",
    "        mel_basis = torch.from_numpy(mel_basis).float()\n",
    "        self.register_buffer('mel_basis', mel_basis)\n",
    "\n",
    "    def spectral_normalize(self, magnitudes):\n",
    "        output = dynamic_range_compression(magnitudes)\n",
    "        return output\n",
    "\n",
    "    def spectral_de_normalize(self, magnitudes):\n",
    "        output = dynamic_range_decompression(magnitudes)\n",
    "        return output\n",
    "\n",
    "    def mel_spectrogram(self, y):\n",
    "        \"\"\"Computes mel-spectrograms from a batch of waves\n",
    "        PARAMS\n",
    "        ------\n",
    "        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n",
    "\n",
    "        RETURNS\n",
    "        -------\n",
    "        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n",
    "        \"\"\"\n",
    "        assert(torch.min(y.data) >= -1)\n",
    "        assert(torch.max(y.data) <= 1)\n",
    "\n",
    "        magnitudes, phases = self.stft_fn.transform(y)\n",
    "        magnitudes = magnitudes.data\n",
    "        mel_output = torch.matmul(self.mel_basis, magnitudes)\n",
    "        mel_output = self.spectral_normalize(mel_output)\n",
    "        return mel_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nw8qr0H1xKfo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import IPython.display as ipd\n",
    "stft = TacotronSTFT()\n",
    "def load_mel(path):\n",
    "    audio, sampling_rate = load_wav_to_torch(path)\n",
    "    if sampling_rate != stft.sampling_rate:\n",
    "        raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "            sampling_rate, stft.sampling_rate))\n",
    "    audio_norm = audio /32768\n",
    "    audio_norm = audio_norm.unsqueeze(0)\n",
    "    audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
    "    melspec = stft.mel_spectrogram(audio_norm)\n",
    "    return melspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhVb61qZyrzR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Tacotron2 and WaveNet  text-to-speech demo.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
